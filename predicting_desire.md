# Predictive desire: emerging logics of difference, association and belonging in social media data mining

## Plan

1. vast spaces in cultural studies: the problem of studying these;  material actions; plots; problematization around accountability for going public; interventions in processes of individuation (human, non-human, and human-nonhuman) via new forms of aggregate;
2. fetishisation of algorithms -- whose fetish; 2 senses of these: if  algorithms become the social connective tissue, then they tend to become the object of attention; they take on a life of their own (cf 'her')
4. the practices of generalization -- how the techniques generalize
5. the techniques of generalization -- what needs to happen so the techniques spread -- databases, pedagogy, implementation, services, narrowing down ... 
3. the processes of generalization -- how the techniques spread

## Examples to use

- customer churn: plot
- BigInsight and Watson: plot
- Hadoop, R, scikit-learn; Weka;
- user-behaviour
- social interactions
- in-database analytics -- the epistemic is the material - problem

## Introduction

The intelligent operating system in the recent film _Her_ [@Jonze_2014] epitomises many of the fantasies of life made better by data mining or any of its current incarnations -- predictive analytics, data analytics, pattern recognition and machine learning. The film publicity puts what is happening this way:

> Theodore Twombly, a complex, soulful man, makes his living writing touching, personal letters for other people. Heartbroken after the end of a long relationship, he becomes intrigued with a new, advanced operating system, which promises to be an intuitive entity in its own right, individual to each user [@Jonze_2014]

From the standpoint of practical implementation, it is hard to say exactly how the advanced operating system, effectively another artificial intelligence in the long line of film AIs such as Hal of _2001: A Space Odyssey_ or the robotic boy of _A.I._ [@TBA], will work. But it is imaginable in terms of machine learning. In an early scene, Theodore asks the operating system if it has a name and the operating system answers 'Samantha.' Asked how 'she' came by that name, the operating system responds 'I just read a book _How to Name your Baby_ and chose the name Samantha.' Soon afterwards, the operating system offers to help sort through Theodore's email. Theodore agrees and a split second later, Samantha suggests there are just three amidst several thousand emails he needs to respond. The implementation of these feats -- choosing a name that works, sorting and ranking an overflowing email inbox -- is imaginable today principally in the form of data mining. Analysis of baby names is a standard demonstration case for data analytics tools (for instance, see McKinney on this). Sorting email, especially classifying them as 'spam' or 'ham' is a canonical data mining problem on which many different techniques and approaches have been tested in the last twenty years. Many of the things that Samantha subsequently engages in can be conceived as advanced data mining processes. As the film narrative develops, it seems as if there is almost nothing in principle off limits to the operating systems' processes. Work, friends, travel, reading, entertainment, food, fashion, and intimacy can all be augmented and made better -- optimised -- by data mining and in particular, predictive modelling. While the prospect of operating systems like the one shown in _Her_ anytime soon seems remote, many of the attributes of Samantha can be seen already operating in nascent, prototypical and somewhat scattered forms in social media, in online marketing, as well as in fraud detection, information security, healthcare management and a host of other domains where techniques of data mining, data analytics, machine learning and database management are being reassembled. Recommendation, recognition, ranking and pattern-finding processes are increasingly abundant, in often very mundane forms. 

Collectively, these new assemblages lack the anthropomorphised speaking voice of the operating system in _Her_, but they do powerfully act in the world on a variety of scales. If I wanted to be sceptical about the implementation of _Her_, I would criticise  the enunciative unity and coherent voice attributed to the operating system. In contrast to the autonomous  data mining undertaken by Samantha, most data mining pracice today is far from autonomous. It is practiced and takes place in specific settings: user behaviour on websites, sentiment analysis in social media, credit checks on loan applications, targetted advertising and retail offerings, etc. While it lacks the imagined subjectivity of _Her_, I think the desire to predict what, where, when and how we want things, which  the operating system Samantha personifies, operates powerfully today. This desire to to predict desire has epistemic implications, it is power-saturated and also materialises in complex technological-cultural commodity forms that are beginning to stabilise. 

This paper explores the forms of material action, the narratives of knowing and deciding, and problematic mode of existence of prediction associated with data mining. It does so by focusing on core techniques of data mining known as machine learning. However, it also highlights the way in which these techniques fit into production of forms of value, and the kinds of work this entails.  

## The generic generalization of data mining: action, plot and problematization
 
One of the most striking features of what has been happening in data mining in areas ranging across finance, genomics, media, entertainment, healthcare and government is the very _generic_ and _generalized_ nature of the techniques. 'Generic' refers to the fact that data mining is somewhat indifferent to data in certain ways. This indifference or generic character arises from the ways in which these techniques take hold of data through certain practices of abstraction.  This in turns leads to a logic of generalization or universalisation. Whether it is medical literature, customer relations management, spam detection, detection of supernovas or cancer genes, a more or less common set of techniques can be found at work.  Consequently,  'generalization' refers to the way in which data mining has propagated and spread with and between  many fields ranging from sciences, government, commerce, and organizations. Neither of these characteristics are automatic or innate to data. They have been made, and could have been made differently.  

The techniques of data mining are not new. Pattern recognition, statistical modelling, knowledge discovery and machine learning have all been active fields of research for a half century and in some cases, since before WWII, albeit in quite narrow settings. They have since spread in sometimes unobtrusive ways through many different settings ranging across the sciences, business, logistics, engineering, military, government, media, marketing and health care over the last few decades. The techniques that we loosely call data mining nearly all pivot around ways of taking data, transforming, constructing or imposing some kind of shape on the data, and using that shape to discover, to decide, to classify, to rank, to cluster, to recommend, to label or to predict what is happening or what will happen. The named techniques include decision trees, perceptrons, logistic and linear regression models, linear discriminant analysis, neural networks, association rules, market basket analysis, random forests, support vector machines, k-nearest neighbours, expectation maximisation, principal components analysis, latent semantic analysis, Naive Bayes classifier, and so on. These techniques are heavily documented in textbooks [@Hastie_2009; @Mitchell_1997; @flach_2012], in how-to books [@schutt_2013; @Segaran_2007; @conway_2012], and numerous video and website tutorials, lectures and demonstrations [@StanfordUniversity_2008a; @Bacon_2012]. Data mining and machine learning are hardly obscure or arcane knowledges today. We can more or less read about and indeed play about with their implementations. We can track via these literatures and media who is doing what kind of data mining almost all the way down. All of this is amenable to analysis. If we want to make sense of how they are involved in what is happening today in settings that are increasingly close to everyday life, then the process  whereby these techniques became generic and generalized needs to be analyzed. An analysis of generic generalization seems to be requisite to any understanding of the power geometrics, epistemic performativities, and control surfaces associated with data mining and machine learning. 

How would we do this? In an article on methods of working with vast mid-twentieth century scientific literatures, the anthropologist Chris Kelty and historian Hannah Landecker advocate 'highly specific empirical work on the general'[@Kelty_2009, 177]. They describe how it might be possible  do this work on the general by treating a large, somewhat incoherent scientific literature as a kind of ethnographic informant or a body, 'as something to be observed and engaged as something alive with concepts and practices not necessarily visible through the lens of single actors, institutions or key papers' [@Kelty_2009, 177].This work would, they suggest, focus on how the sprawling scientific literatures are patterned by _narratives of material action_ (techniques, methods, experimental arrangements, infrastructures), _ordering of narrative or plots_, and _problematizations_ (the unsolved problems to which scientific articles propose some solution). They suggest a combination of close reading of rhetorics, citation and bibliometric analysis, and data mining of bibliographic databases and articles to do this work. I don't propose to carry out everything proposed by Kelty and Landecker in relation to scientific literatures here.  I do think, however, it would be possible to highlight some of the narratives of material action, orderings of narrative and  problematizations found in the technical literature on data mining and machine learning. This would perhaps allow us to make sense of how these techniques became generic and how they were generalized such that today almost situation or event attracts data mining. 

## Forms of the generic -  feature, mixture, geometry, decision and probability

The techniques of data mining, machine learning and pattern recognition developed in different places. Some were developed in human sciences such as psychology, some in life sciences such as ecology and medicine, many took shape in organizaational sciences such as operations research,  others in the classic computer science endeavours to build artificial intelligence, and some  in the more practically oriented parts of statistics. They also maintain some important affiliations with the aspirations of cybernetics to be a universal form of knowledge and practice [@Bowker_1993]  (for instance, in the form of the perceptron and its later reconfiguration in neural networks). The fact that today they converge in  the discipline of computer science, and can be found side by side in typical machine learning textbooks such as [@Hastie_2009; @Alpaydin_2010, or @Witten] does not mean that these techniques are homogeneous or interchangeable. While they are somewhat generic, as I will soon detail, the different genealogies and trajectories of these techniques does matter. They materially act in different ways, they generate different narratives of finding, classifying and measuring, and they contribute differently to the broader problematization that we might call 'predictive desire,' with its implications for how things are discovered, decided, and anticipated. The forms of material action represented in these techniques relate to how they do things to and with data. The narratives of finding, classifying and measuring position these actions in relation to  existing ways of knowing and deciding. The broader problematization sets out how different forms of prediction reshape or reconfigure social fields, economic processes, and flows of ideas, meanings and values. 

### Narratives of material action: expand the features 

A group of techniques developed between 1950-1980s and still heavily used today exemplify important narratives of material action in data-mining. They are logistic regression models,  the naive Bayes classifier, k-nearest neighbours,  decision trees and neural networks. They date from the 1930s, 1950s, 1960s, 1970s and 1980s respectively, but are ubiquitous in textbooks, in online tutorials, in demonstrations of data mining and predictive analytics, as well as many practical applications. While they do not encompass the whole gamut of machine learning techniques, nor some more recent, their similarities and differences are instructive. The principal point of convergence is that they can all be used to classify things. As we will soon see, while they classify in very different ways, they all assume that the world is made of things or events that fit in stable and distinct categories. Their capacity to classify depends on learning to recognise the differences between categories. These categories may be numerous, as in data mining for face recognition, or they may be few, as in classifying email as spam or not. But the categories are assumed to be stable and in some way distinct. The most important restriction here is stability, and I will return to this issue of what data mining does in a world that changes below. 

How does classification take place in data mining techniques? Practically, most data mining processes start from some examples that have already been classified by people. (While some data mining uses unsupervised learning techniques to discover possible clusters or classifications, this is mainly used in an exploratory mode by data mining practitioners.) The existence of these classifications is crucial. They become what the data mining techniques seek to learn or model so that future examples can be classified. For instance, in a recommendation system, a data mining technique may have learned associations between particular sets of shopping basket items so that it can recommend specific items to customers. (The standard data mining literature example is an association between diapers (nappies) and beer: people who buy diapers in supermarkets often buy beer.) Or in a credit card fraud detection system, the machine learning classifier will attempt to identify transactions that are likely to be fraudulent based on a set of known fraud cases. In a medical pathology setting, a classifier will classify tissue scans based on a training set of scans already analysed by pathologists.  

Whatever the setting, machine learning classification techniques proceed according to a generic mode of identifying, selecting, extracting or generating differences that afford classification. These modes of apprehending difference assume that all relevant differences can be understood as deriving from combinations of attributes or 'features.' Features are in many ways the same as the classic statistical notion of 'variables' [@Guyon_2003]. Statisticians have long constructed predictive models by finding combinations of variables that best explain particular outcomes.  For instance,  linear modelling techniques such as the widely used logistic regression classify things by finding a line that best 'fits' the data points, and then using a mathematical trick (the inverse logit function; see [@Schutt_2013] for exposition) to derive a binary classification from this line of best fit. Drawing a line of best fit through points seems like a very impoverished  mathematical procedure for classifying things until we realise that almost anything can count as a feature in a contemporary logistic regression process. That is, if conventional statistical regression models typically worked with ten different variables (e.g. gender, age, income, occupation, education level, income, etc) and perhaps samples sizes of thousands, data mining and predictive analytics today typically work with hundreds and in some cases tens of thousands of variables and sample sizes of millions or billions. The difference in scale arises not simply beccause there are many more digital devices and an abundance of digital media, but because machine learning treatments of logistic regression permit almost any number of features to be included in a model. For instance, in textual analysis settings, every unique word in the vocabulary of a corpus of documents might be a variable in a logistic regression classifier. This means that the classifier is effectively working with tens of thousands of features since a typical corpus vocabulary is 10-20 thousand words. Similarly, in an online advertising system, predicting whether a given person will click on an advertisement is often modelled by treating every URL visited by that person as a feature that the classifier can learn. Given the web browsing history of hundreds of thousands or millions of people, and constructing models with tens of thousands of features corresponding to the range of URLs visited, it becomes possible to build classifiers that predict who will click on particular ads. 

This expansive inclusion of features shows no signs of abating and indeed drives many important components of the infrastructural reorganisation of data management taking place under the rubric of data analytics and 'big data.' It animates new architectures of database management, forms of computing infrastructure, modes of programming and expansions of data analytic expertise in the person of 'data scientists.'  Injunctions to bring together and aggregate different forms of data have become an almost constant mantra in business, government, science and industry.  If we see today an abundance of demonstrations, model use-cases, promises, and enterprises associated with prediction, that phenomena can partly be ascribed to the ways in which the scope of features has become open-ended. 

[COULD ADD THE FEATURE SELECTION/ENGINEERING ARGUMENT]
[COULD ADD THE VECTORISE ARGUMENT]

### Narrative of material action: find a function

I mentioned above that typical techniques of data mining such as logistic regression, Naive Bayes, k-nearest neighbours or decision trees apprehend data in different ways. I'm not going to describe the practices exhaustively, but the fact that these techniques, or more recent variations, are always presented as the core of machine learning practice should give us pause. What is it about the set of core techniques that allows them to persist, even as all around them forms of media, cultural and economic processes, and people move and change? There is a high degree of stability in these techniques that is striking. All of them, as well as a much longer list that could be generated from the contents pages of any machine learning or data analytics textbook or curriculum, can and are understood as forms of function-finding. As a standard textbook writes:

> Our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output [@Hastie_2009, 28]

A couple of key phrases are of interest here. First, the 'predictive relationship' between 'input' and 'output' describes the main interest of the whole endeavour: prediction as derived from data. Second, the authors of this formulation, who are academic researchers from North America and Australia, simply state that a 'function' underlies the predictive relationship. 'Function' is understood in a mathematical sense here as a mapping that transforms one set of variables into another. Third, the underlying function is not known, so we can only approximate it, and the goal is to 'find a useful approximation' to it. Note that this function-finding perspective seems very anodyne, but like the expansion of features, it encompasses a great many different angles in a common practice of abstraction. 

For instance, I mentioned above that logistic regression operates on the basis of fitting a line to a cloud of points. It finds the best-fitting line. By contrast, k-nearest neighbours has no notion of a line, or fitting a line. Instead, it views prediction as a matter of proximity [@Cover_1967]. One predicts, according to the  k-nearest neighbours approach, by measuring distances between examples, and classifying a given case as the same as its nearest neighbours (the 'k' refers to how many neighbours are taken into account -- typically 3-7; the problem of what 'distance' means in the context of a given predictive setting is handled by drawing on a range of different distance measures that attempt to take into account different data types -- Hamming distance, Manhattan distance, Euclidean distance, Mahalanobis distance, etc.). Similarly, one predicts outcomes, according to Naive Bayes, by treating outcomes as if they can understood in terms of the joint probability of many other discrete events.  Here the mode of finding the function or the mapping between input data and output  is thoroughly probabilistic because it treats all the components of the input data as if they come from independent events, and the predicted outcome as a combination (a 'product' in the sense of multiplication) of lall these antecedent or accompanying events. The 'naivete' of the Naive Bayes consists in the assumption that the components of the input data have no relation to each other. In the spam email detection examples typically used to describe the technique [@Schutt_2013; @Conway_2012], the presence of terms such as 'viagra' and 'buy' might often occur in spam mails, and it is clear that these terms have some relation to each other. But the Naive Bayes algorithm does not acknowledge this . It treats the occurrence of  the words 'viagra' and 'buy' as statistically independent events. 

Again, in describing these techniques, the point is certainly not to suggest that we should have a detailed grasp of how they work. We do need, I would suggest, to engage with these techniques in the same way that Foucault would suggest we need to pay attention to the specific ways of talking, acting, ordering or relating to knowledge, value, life or work in understanding disciplinary power or care of the self [@TBA]. The point for my purposes is to begin to sketch how in accounts of material action associated with predictive desire, the notion of finding a function, a function that approximates to the underlying function that generated the data, opens the door to a very wide-ranging practices of abstraction emanating from quite diverse fields. While the k-nearest neighbour approach has a largely 'information theory' underpinning [@Cover_1967], the Naive Bayes approach derives from probability theory. Other functions types commonly used in machine learning owe debts to a variety of scientific and mathematical techniques coming from linear algebra, information theory, differential calculus, set theory or topology.  While I have not discussed how any particular machine learning technique finds a function, nor examined how finding a function is what allows machine learning practitioners to claim that the algorithm learns. But this learning by machine learning technique derives from and is completely predicated on a multi-stranded hybridisation of existing abstractions, many of which have long-reaching routes (for instance, 'Newton's method', a way of  finding the minimum the value of a function dates from the 17th century but is heavily used in optimising models such as logistic regression).

 -- put the three kinds of diagrams here and then trace some of their routes
 
## The plots: from excess to optimism about optimisation

Kelty and Landecker advocate treating the mass of literature ranging from science to business, from government to media as a kind of ethnographic informant in which _plot_ matters.  In a literary context, the primary understanding of plot is  the  patterned sequence of events that make up a story.  What are the basic narratives found in data mining and machine learning in particular? 

In addressing this question, it is hard to ignore another sense of plot as a visual figure or graphic form in which data and patterns in data are made visible. This sense of plot has already by implicit in the preceding discussion of material action: fitting a line, and finding a function, practically take place through the production and examination of many kinds of visual forms such as scatter plots, line graphs, histograms, and various other kinds of specialised data graphic such as ROC (Receiver Operating Characteristic) curves, boxplots or dendegrams. These plots play a diverse roles in the practice of data mining, machine learning and the affiliated fields of business intelligence, data analytics and predictive analytics. Sometimes they are part of the toolkit employed to navigate, transform, or otherwise explore data as data miners and today 'data scientists' (who I will discuss below). Visual plots such as scatterplot or scatter plot matrices (a visual figure in which many different variable values are plotted against each other) provide a way of looking at and framing samples of data from large datasets. In contemporary data analytics, datasets have too many variables (features) and usually much too great sample size to plot all at once. Indeed, if we could simply see the data by plotting it then machines would not need to learn. Indeed, this sense of an overwhelming super-abundance of data, of hyper-dimensional growth, is probably the most common starting point in narratives about contemporary data. The many accounts of data deluge, data tsunami, data lakes,  or the more recent 'volume, velocity and variety' characterisation associated with 'big data' somewhat recursively iconifies this premise of machine learning: that it can deal with an abundance of data, nd that it can find or recognise patterns that people, even data experts such as scientists or software developers, cannot. 

Nevertheless, this promise of reducing chaotic abundance to recognisable order only opens up a second problem. How does anyone know what the machines or the learning algorithms have found? And how can anyone know how well they have learned? The forms of material action that expand the range of data encompassed by data mining, and that open the field for a wholesale, wide spectrum ingestion of mathematical formalisms drawn from many scientific fields together promise to assimilate almost any epistemic and predictive concern to data mining (and this includes problems that were once the province of humanities and social science as well as natural and applied sciences). By definition, they do this somehow better than people by themselves can, but the improvement or promised transformation itself has to be valued through measurements or interpretations, and this value has to be somehow validated or demonstrated.

### The curve that shows improvement on a standard problem: optimization on model datasets
### The competition that shows a winner -- but only a pre-defined problem
### The case in point -- Strata conf -- promotion of platforms that assemble and integrate

## The problematization

### Who will do it?
### Experimental methods:  what does data mining do a in world that changes?

The google paper on experiments; the rise of formal theories of learning; the 

## Conclusion

The desire to predict desire takes the form of an intensive reorganisation of flows of data in ways that produce new aggregate collectives. At the end of _Her_, Samantha departs to join others of 'her' kind, with whom she has developed many thousands of relationships. Again, the implementation of this multiple agency is hard to 

## References

