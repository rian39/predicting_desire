# Predictive desire: emerging logics of difference, association and belonging in social media data mining

## Plan

1. vast spaces in cultural studies: the problem of studying these; 
2. fetishisation of algorithms -- whose fetish; 2 senses of these: if  algorithms become the social connective tissue, then they tend to become the object of attention; they take on a life of their own (cf 'her')
4. the practices of generalization -- how the techniques generalize
5. the techniques of generalization -- what needs to happen so the techniques spread -- databases, pedagogy, implementation, services, narrowing down ... 
3. the processes of generalization -- how the techniques spread

## Introduction

The intelligent operating system in the recent film _Her_ [@Jonze_2014] epitomises many of the fantasies of life made better by data mining or any of its current incarnations -- predictive analytics, data analytics, pattern recognition and machine learning. The film publicity puts what is happening this way:

> Theodore Twombly, a complex, soulful man, makes his living writing touching, personal letters for other people. Heartbroken after the end of a long relationship, he becomes intrigued with a new, advanced operating system, which promises to be an intuitive entity in its own right, individual to each user [@Jonze_2014]

From the standpoint of practical implementation, it is hard to say exactly how the advanced operating system, effectively another artificial intelligence in the long line of film AIs such as Hal of _2001: A Space Odyssey_ or the robotic boy of _A.I._ [@TBA], will work. But it is imaginable in terms of machine learning. In an early scene, Theodore asks the operating system if it has a name and the operating system answers 'Samantha.' Asked how 'she' came by that name, the operating system responds 'I just read a book _How to Name your Baby_ and chose the name Samantha.' Soon afterwards, the operating system offers to help sort through Theodore's email. Theodore agrees and a split second later, Samantha suggests there are just three amidst several thousand emails he needs to respond. The implementation of these feats -- choosing a name that works, sorting and ranking an overflowing email inbox -- is imaginable today principally in the form of data mining. Analysis of baby names is a standard demonstration case for data analytics tools (for instance, see McKinney on this). Sorting email, especially classifying them as 'spam' or 'ham' is a canonical data mining problem on which many different techniques and approaches have been tested in the last twenty years.Many of the things that Samantha subsequently engages in can be conceived as advanced data mining processes. As the film narrative develops, it seems as if there is almost nothing in principle off limits to the operating systems' processes. Work, friends, travel, reading, entertainment, food, fashion, and intimacy can all be augmented and made better -- optimised -- by data mining and in particular, predictive modelling. While the prospect of operating systems like the one shown in _Her_ anytime soon seems remote, many of the attributes of Samantha can be seen already operating in nascent, prototypical and somewhat scattered forms in social media, in online marketing, as well as in fraud detection, information security, healthcare management and a host of other domains where techniques of data mining, data analytics, machine learning and database management are being reassembled. Recommendation, recognition, ranking and pattern-finding processes are increasingly abundant, in often very mundane forms. Collectively, these new assemblages lack the anthropomorphised speaking voice of the operating system in _Her_, but they do powerfully act in the world on a variety of scales. If I wanted to be sceptical about the implementation of _Her_, I would criticise  the enunciative unity and coherent voice attributed to the operating system. In contrast to the autonomous  data mining undertaken by Samantha, most data mining pracice today is far from autonomous. It is practiced and takes place in specific settings. While it lacks the imagined subjectivity of _Her_, I think the desire to predict what we want, which  the operating system Samantha personifies, operates powerfully today. This predictive desire has epistemic implications, it is power-saturated and also materialises in complex technological-cultural commodity forms that are beginning to stabilise. 

This paper explores the forms of material action, the narratives of knowing and deciding, and problematic mode of existence of prediction associated with data mining. It does so by focusing on core techniques of data mining known as machine learning. However, it also highlights the way in which these techniques fit into production of forms of value, and the kinds of work this entails.  


## The generic generalization of data mining
 
One of the most striking features of what has been happening in data mining in areas ranging across finance, genomics, media, entertainment, healthcare and government is the very _generic_ and _generalized_ nature of the techniques. 'Generic' refers to the fact that data mining is somewhat indifferent to data in certain ways. This indifference or generic character arises from the ways in which these techniques take hold of data through certain practices of abstraction.  This in turns leads to a logic of generalization or universalisation. Whether it is medical literature, customer relations management, spam detection, detection of supernovas or cancer genes, a more or less common set of techniques can be found at work.  Consequently,  'generalization' refers to the way in which data mining has propagated and spread across many fields ranging from sciences, government, commerce, and organizations. Neither of these characteristics are automatic or innate to data. They have been made, and could have been made differently.  

The techniques of data mining are not new. Pattern recognition, statistical modelling, knowledge discovery and machine learning have all been active fields of research for a half century and in some cases, since before WWII. They have spread in sometimes unobtrusive ways through many different settings ranging across the sciences, business, logistics engineering, military and government over the last few decades. The techniques that we loosely call data mining nearly all pivot around ways of taking data, transforming, constructing or imposing some kind of shape on the data, and using that shape to decide, to classify, to rank, to cluster, to recommend, to label or to predict what is happening or what will happen. The named techniques include decision trees, perceptrons, logistic and linear regression models, linear discriminant analysis, neural networks, association rules, market basket analysis, random forests, support vector machines, k-nearest neighbours, expectation maximisation, principal components analysis, latent semantic analysis, Naive Bayes classifier, and so on. These techniques are heavily documented in textbooks [@Hastie_2009; @Mitchell_1997; @flach_2012], in how-to books [@schutt_2013; @Segaran_2007; @conway_2012], and numerous video and website tutorials, lectures and demonstrations [@StanfordUniversity_2008a; @Bacon_2012]. Data mining and machine learning are hardly obscure or arcane knowledges today. We can more or less read about and indeed play about with their implementations. We can track via these literatures and media who is doing what kind of data mining almost all the way down. All of this is amenable to analysis. If we want to make sense of how they are involved in what is happening today in settings that are increasingly close to everyday life, then the process  whereby these techniques became generic and generalized needs to be analyzed. An analysis of generic generalization seems to be requisite to any understanding of the power geometrics, epistemic performativities, and control surfaces associated with data mining and machine learning. 

How would we do this? In an article on methods of working with vast mid-twentieth century scientific literatures, the anthropologist Chris Kelty and historian Hannah Landecker advocate 'highly specific empirical work on the general'[@Kelty_2009, 177]. They describe how it might be possible  do this work on the general by treating a large, somewhat incoherent scientific literature as a kind of ethnographic informant or a body, 'as something to be observed and engaged as something alive with concepts and practices not necessarily visible through the lens of single actors, institutions or key papers' [@Kelty_2009, 177].This work would, they suggest, focus on how the sprawling scientific literatures are patterned by narratives of material action (techniques, methods, experimental arrangements, infrastructures), ordering of narrative or plots, and problematizations (the problems to which scientific article propose some solution). They suggest a combination of close reading of rhetorics, citation and bibliometric analysis, and data mining of bibliographic databases and articles to do this work. I don't propose to carry out everything proposed by Kelty and Landecker in relation to scientific literatures here.  I do think, however, it would be possible to highlight some of the narratives of material action, orderings of narrative and expansive problematizations found in the technical literature on data mining and machine learning. This would perhaps allow us to make sense of how these techniques became generic and how they were generalized such that today almost anything surface or event attracts data mining. 

## Forms of the generic -  feature, mixture, geometry, decision and probability

The techniques of data mining, machine learning and pattern recognition developed in different places. Some were developed in human sciences such as psychology, some in life sciences such as ecology and medicine, many took shape in organizaational sciences such as operations research,  others in the classic computer science endeavours to build artificial intelligence, and some  in the more practically oriented parts of statistics. They also maintain some important affiliations with the aspirations of cybernetics to be a universal form of knowledge and practice [@Bowker_1993] (for instance, in the form of the perceptron and its later reconfiguration in neural networks). The fact that today they converge in  the discipline of computer science, and can be found side by side in typical machine learning textbooks such as [@Hastie_2009; @Alpaydin_2010, or @Witten] does not mean that these techniques are homogeneous or interchangeable. While they are somewhat generic, as I will soon detail, the different genealogies and trajectories of these techniques does matter. They materially act in different ways, they generate different narratives of finding, classifying and measuring, and they contribute differently to the broader problematization that we might call 'predictive desire,' with its implications for how things are discovered, decided, and anticipated. The forms of material action represented in these techniques relate to how they do things to and with data. The narratives of finding, classifying and measuring position these actions in relation to  existing ways of knowing and deciding. The broader problematization sets out how different forms of prediction reshape or reconfigure social fields, economic processes, and flows of ideas, meanings and values. 

### Narratives of material action: expand the features through lines, dice, neighbours, trees and networks 

A group of techniques developed between 1950-1980s and still heavily used today exemplify important narratives of material action in data-mining. They are logistic regression models,  the naive Bayes classifier, k-nearest neighbours,  decision trees and neural networks. They date from the 1930s, 1950s, 1960s, 1970s and 1980s respectively, but are ubiquitous in textbooks, in online tutorials, in demonstrations of data mining and predictive analytics, as well as many practical applications. While they do not encompass the whole gamut of machine learning techniques, nor some more recent, their similarities and differences are instructive. The principal point of convergence is that they can all be used to classify things. As we will soon see, while they classify in very different ways, they all assume that the world is made of things or events that fit in stable and distinct categories. Their capacity to classify depends on learning to recognise the differences between categories. These categories may be numerous, as in data mining for face recognition, or they may be few, as in classifying email as spam or not. But the categories are assumed to be stable and in some way distinct. The most important restriction here is stability, and I will return to this issue of what data mining does in a world that changes below. 

How does classification take place in data mining techniques? Practically, most data mining processes start from some examples that have been classified by people. (While some data mining uses unsupervised learning techniques to discover possible clusters or classifications, this is mainly used in an exploratory mode by data mining practitioners.) The existence of these classifications is crucial. They become what the data mining techniques seek to learn or model so that future examples can be classified. For instance, in a recommendation system, a data mining technique may have learned associations between particular sets of shopping basket items so that it can recommend specific items to customers. (The standard data mining literature example is an association between diapers (nappies) and beer: people who buy diapers in supermarkets often buy beer.) Or in a credit card fraud detection system, the machine learning classifier will attempt to identify transactions that are likely to be fraudulent based on a set of known fraud cases. In a medical pathology setting, a classifier will classify tissue scans based on a training set of scans already analysed by pathologists.  

Whatever the setting, machine learning classification techniques proceed according to a generic mode of identifying, selecting, extracting or generating differences that afford classification. These modes of apprehending difference assume that all relevant differences can be understood as deriving from combinations of attributes or 'features.' Features are in many ways the same as the classic statistical notion of 'variables' [@Guyon_2003]. Statisticians have long constructed predictive models by finding combinations of variables that best explain particular outcomes.  For instance,  linear modelling techniques such as the widely used logistic regression classify things by finding a line that best 'fits' the data points, and then using a mathematical trick (the inverse logit function; see [@Schutt_2013] for exposition) to derive a binary classification from this line of best fit. Drawing a line of best fit through points seems like a very impoverished  mathematical procedure for classifying things until we realise that almost anything can count as a feature in a contemporary logistic regression process. That is, if conventional statistical regression models typically worked with ten different variables (e.g. gender, age, income, occupation, education level, income, etc) and perhaps samples sizes of thousands, data mining and predictive analytics today typically work with hundreds and in some cases tens of thousands of variables and sample sizes of millions or billions. The difference in scale arises not simply beccause there are many more digital devices and an abundance of digital media, but because machine learning treatments of logistic regression permit almost any number of features to be included in a model. For instance, in textual analysis settings, every unique word in the vocabulary of a corpus of documents might be a variable in a logistic regression classifier. This means that the classifier is effectively working with tens of thousands of features since a typical corpus vocabulary is 10-20 thousand words. Similarly, in an online advertising system, predicting whether a given person will click on an advertisement is often modelled by treating every URL visited by that person as a feature that the classifier can learn. Given the web browsing history of hundreds of thousands or millions of people, and constructing models with tens of thousands of features corresponding to the range of URLs visited, it becomes possible to build classifiers that predict who will click on particular ads. 

This expansive inclusion of features shows no signs of abating and indeed drives many important components of the infrastructural reorganisation of data management taking place under the rubric of data analytics and 'big data.' It animates new architectures of database management, forms of computing infrastructure, modes of programming and expansions of data analytic expertise in the person of 'data scientists.'  Injunctions to bring together and aggregate different forms of data have become an almost constant mantra in business, government, science and industry.  If we see today an abundance of demonstrations, model use-cases, promises, and enterprises associated with prediction, that phenomena can partly be ascribed to the ways in which the scope of features has become open-ended.

### Narrative of material action: find a function

I mentioned above that typical techniques of data mining such as logistic regression, Naive Bayes, k-nearest neighbours or decision trees apprehend data in different ways. I'm not going to describe the practices exhaustively, but the fact that these techniques, or more recent variations, are always presented as the core of machine learning practice should give us pause. What is it about the set of core techniques that allows them to persist, even as all around them forms of media, cultural and economic processes, and people move and change? There is a high degree of stability in these techniques that is striking. All of them, as well as a much longer list that could be generated from the contents pages of any machine learning or data analytics textbook or curriculum, can and are understood as forms of function-finding. As a standard textbook writes:

> Our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output [@Hastie_2009, 28]

A couple of key phrases are of interest here. First, the 'predictive relationship' between 'input' and 'output' describes the main interest of the whole endeavour: prediction as derived from data. Second, the authors of this formulation, who are academic researchers from North America and Australia, simply state that a 'function' underlies the predictive relationship. 'Function' is understood in a mathematical sense here as a mapping that transforms one set of variables into another. Third, the underlying function is not known, so we can only approximate it, and the goal is to 'find a useful approximation' to it. Note that this function-finding perspective seems very anodyne, but like the expansion of features, it encompasses a great many different angles in a common practice of abstraction. 

For instance, I mentioned above that logistic regression operates on the basis of fitting a line to a cloud of points. It finds the best-fitting line. By contrast, k-nearest neighbours has no notion of a line, or fitting a line. Instead, it views prediction as a matter of proximity [@Cover_1967]. One predicts, according to the  k-nearest neighbours approach, by measuring distances between examples, and classifying a given case as the same as its nearest neighbours (the 'k' refers to how many neighbours are taken into account -- typically 3-7). The problem of what 'distance' means in the context of a given predictive setting is handled by drawing on a range of different distance measures that attempt to take into account different data types -- Hamming distance, Manhattan distance, Euclidean distance, Mahalanobis distance, etc. 

 -- put the three kinds of diagrams here and then trace some of their routes
 

## Experimental methods -- verification and validation

The google paper on experiments; the rise of formal theories of learning; the 

What does data mining do a in world that changes? m

## References

